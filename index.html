<!DOCTYPE html>
<html>
<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-127641291-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());
        gtag('config', 'UA-127641291-1');
    </script>

    <meta charset=UTF-8>
    <meta name=description content="Changhan Wang">
    <meta name=keywords content="Changhan Wang">
    <title>Changhan Wang</title>

    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css"
          integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css"
          integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
    <link rel="stylesheet" href="style.css" type="text/css">
    <!-- for FF, Chrome, Opera -->
    <link rel="icon" type="image/png" href="favicons/favicon16x16.png" sizes="16x16">
    <link rel="icon" type="image/png" href="favicons/favicon32x32.png" sizes="32x32">
    <!-- for IE -->
    <link rel="icon" type="image/x-icon" href="favicons/favicon32x32.ico">
    <link rel="shortcut icon" type="image/x-icon" href="favicons/favicon32x32.ico">
</head>

<body>
<div id="sidebar">
    <div style="margin-top: 28%"><img src="changhan.png" width=50%></div>
    <div style="margin-top: 10%"><h3>Changhan Wang<br/>（王昌翰）</h3></div>
    <div style="text-align: left">
        <ul class="fa-ul" style="margin-top: 12%">
            <li class="tight-li"><i class="fa fa-li fa-map-marker"></i>Facebook AI Research (FAIR)</li>
            <li class="tight-li"><i class="fa fa-li fa-map-marker"></i>New York City, USA</li>
            <li class="tight-li"><i class="fa fa-li fa-envelope"></i>wangchanghan AT gmail</li>
            <li class="tight-li">
                <a href="https://www.linkedin.com/in/changhanwang" title="LinkedIn" target="_blank">
                    <i class="fab fa-li fa-linkedin"></i>LinkedIn</a>
            </li>
            <li class="tight-li">
                <a href="https://scholar.google.com/citations?user=IQhdjAMAAAAJ&hl=en" title="Google Scholar"
                   target="_blank"><i class="fa fa-li fa-graduation-cap"></i>Google Scholar</a>
            </li>
            <li class="tight-li">
                <a href="https://github.com/kahne" title="GitHub" target="_blank">
                    <i class="fab fa-li fa-github"></i>GitHub</a>
            </li>
            <li class="tight-li">
                <a href="https://www.facebook.com/wangchanghan" title="Facebook" target="_blank">
                    <i class="fab fa-li fa-facebook"></i>Facebook</a>
            </li>
            <li class="tight-li">
                <a href="https://www.instagram.com/changhan.w" title="Instagram" target="_blank">
                    <i class="fab fa-li fa-instagram"></i>Instagram</a>
            </li>
            <li class="tight-li">
                <a href="https://twitter.com/ChanghanWang" title="Twitter" target="_blank">
                    <i class="fab fa-li fa-twitter"></i>Twitter</a>
            </li>
        </ul>
    </div>
</div>

<div id="content">
    <div class="content-section">
        <h2>About Me</h2>
        I am a Research Engineer at <a href="https://research.fb.com/category/facebook-ai-research" target="_blank">
        Facebook AI Research (FAIR)</a>, mainly working on natural language and speech processing.
        Prior to Facebook, I was a Software Engineer at <a href="https://www.tapad.com" target="_blank">Tapad</a>.
        I earned my Master's degree in Computer Science from <a href="http://umich.edu" target="_blank">University of
        Michigan, Ann Arbor</a>, where I worked with
        <a href="https://web.eecs.umich.edu/~honglak/" target="_blank">Prof. Honglak Lee</a> on analyzing medical images
        with deep learning. I hold a Bachelor's degree (Hons) in Mathematics from
        <a href="http://www.zju.edu.cn/english/" target="_blank">Zhejiang University</a>.
    </div>

    <div class="content-section">
        <h2>What's New?</h2>
        <ul class="list-no-bullet">
<!--            <li class="tight-li">-->
<!--                <strong>2020-06-11</strong>:-->
<!--                New arXiv <a href="https://arxiv.org/pdf/2006.05474.pdf" target="_blank">paper</a>.-->
<!--            </li>-->
            <li class="tight-li">
                <strong>2020-02-11</strong>:
                One <a href="https://arxiv.org/pdf/2002.01320.pdf" target="_blank">paper</a> was accepted to
                <strong>LREC 2020</strong>! <a href="https://github.com/facebookresearch/covost" target="_blank">Data & code</a>
                is available.
            </li>
            <li class="tight-li">
                <strong>2019-11-10</strong>:
                One <a href="https://arxiv.org/pdf/1909.03341.pdf" target="_blank">paper</a> was accepted to
                <strong>AAAI 2020</strong>! <a href="https://github.com/pytorch/fairseq/tree/master/examples/byte_level_bpe" target="_blank">Code</a>
                is available.
            </li>
            <li class="tight-li">
                <strong>2019-11-03</strong>:
                <a href="https://facebookresearch.github.io/vizseq" target="_blank">VizSeq</a>
                (an analysis toolkit for natural language generation) released! Check out our
                <a href="https://ai.facebook.com/blog/vizseq-a-visual-analysis-toolkit-for-accelerating-text-generation-research" target="_blank">blog</a>.
            </li>
            <li class="tight-li">
                <strong>2019-09-03</strong>:
                One <a href="https://arxiv.org/pdf/1905.11006.pdf" target="_blank">paper</a> was accepted to
                <strong>NeurIPS 2019</strong>!
                <a href="https://github.com/pytorch/fairseq/tree/master/examples/nonautoregressive_translation" target="_blank">Code</a>
                is available.
            </li>
            <li class="tight-li">
                <strong>2019-08-12</strong>:
                One <a href="https://www.aclweb.org/anthology/D19-3043.pdf" target="_blank">demo paper</a> was accepted
                to <strong>EMNLP 2019</strong>!
                <a href="https://github.com/facebookresearch/vizseq" target="_blank">Code</a> is available.
            </li>
            <li class="tight-li">
                <strong>2019-05-06</strong>:
                One <a href="https://arxiv.org/pdf/1906.02659.pdf" target="_blank">paper</a> was accepted to
                <strong>CVPR 2019 FATE/CV & CV4GC workshops</strong>! Check out our
                <a href="https://ai.facebook.com/blog/new-way-to-assess-ai-bias-in-object-recognition-systems/" target="_blank">blog</a>.
            </li>
            <li class="tight-li">
                <strong>2018-08-10</strong>:
                One <a href="https://arxiv.org/pdf/1804.07983.pdf" target="_blank">paper</a> was accepted to
                <strong>EMNLP 2018</strong>!
                <a href="https://github.com/facebookresearch/DME" target="_blank">Code</a> is available.
            </li>
            <li class="tight-li">
                <strong>2018-05-04</strong>:
                Our <a href="https://www.aclweb.org/anthology/W18-3221.pdf" target="_blank">system</a> won the
                <strong>1st place</strong> in MS Arabic-Egyptian code-switched Named Entity Recognition shared task
                (<strong>ACL 2018 CALCS workshop</strong>).
            </li>
        </ul>
    </div>

    <div class="content-section">
        <h2>OSS Projects</h2>
        <ul class="list-no-bullet">
            <li>
                <a href="https://facebookresearch.github.io/vizseq" target="_blank">
                    <strong>VizSeq</strong></a>
                <a href="https://github.com/facebookresearch/vizseq" target="_blank">
                    <img style="margin-left: 0.8%"
                         src="https://img.shields.io/github/stars/facebookresearch/vizseq?style=flat-square" /></a>
                <br/>
                <i>An analysis toolkit for natural language generation.</i>
                <a href="https://ai.facebook.com/blog/vizseq-a-visual-analysis-toolkit-for-accelerating-text-generation-research" target="_blank">
                <i class="fas fa-file-alt" style="margin-left: 1%"></i> Blog</a><br/>
            </li>

            <li>
                <a href="https://facebookresearch.github.io/ClassyVision" target="_blank">
                    <strong>ClassyVision</strong></a>
                <a href="https://github.com/facebookresearch/ClassyVision" target="_blank">
                    <img style="margin-left: 0.8%"
                         src="https://img.shields.io/github/stars/facebookresearch/ClassyVision?style=flat-square" /></a>
                <br/>
                <i>An end-to-end PyTorch framework for image and video classification.</i>
            </li>
        </ul>
    </div>

    <div class="content-section" style="margin-bottom: 3rem">
        <h2>Publications</h2>
        <ul id="publicationList" class="list-no-bullet"></ul>
    </div>
</div>

<script src="https://code.jquery.com/jquery-3.4.1.min.js"
        integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js"
        integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous">
</script>
<script src="https://code.jquery.com/jquery-3.4.1.slim.min.js"
        integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n" crossorigin="anonymous">
</script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous">
</script>
<script src="changhan.js"></script>
<script>
    addPaperItem(
        "arXiv", "Improving Cross-Lingual Transfer Learning for End-to-End Speech Recognition with Speech Translation",
        ["Me"], ["Juan Pino", "Jiatao Gu"],
        "arXiv:2006.05474, 2020",
        `Transfer learning from high-resource languages is known to be an efficient way to improve end-to-end
        automatic speech recognition (ASR) for low-resource languages. Pre-trained or jointly trained encoder-decoder
        models, however, do not share the language modeling (decoder) for the same language, which is likely to be
        inefficient for distant target languages. We introduce speech-to-text translation (ST) as an auxiliary task to
        incorporate additional knowledge of the target language and enable transferring from that target language.
        Specifically, we first translate high-resource ASR transcripts into a target low-resource language, with which
        a ST model is trained. Both ST and target ASR share the same attention-based encoder-decoder architecture and
        vocabulary. The former task then provides a fully pre-trained model for the latter, bringing up to 24.6% word
        error rate (WER) reduction to the baseline (direct transfer from high-resource ASR). We show that training ST
        with human translations is not necessary. ST trained with machine translation (MT) pseudo-labels brings
        consistent gains. It can even outperform those using human labels when transferred to target ASR by leveraging
        only 500K MT examples. Even with pseudo-labels from low-resource MT (200K examples), ST-enhanced transfer brings
        up to 8.9% WER reduction to direct transfer.`,
        "static/interspeech2020.png", "https://arxiv.org/pdf/2006.05474.pdf"
    );
    addPaperItem(
        "lrec2020", "CoVoST: A Diverse Multilingual Speech-To-Text Translation Corpus", ["Me"],
        ["Juan Pino", "Anne Wu", "Jiatao Gu"],
        "To appear in Proceedings of the Twelfth International Conference on Language Resources and Evaluation (LREC), 2020",
        `Spoken language translation has recently witnessed a resurgence in popularity, thanks to the development of
        end-to-end models and the creation of new corpora, such as Augmented LibriSpeech and MuST-C. Existing datasets
        involve language pairs with English as a source language, involve very specific domains or are low resource.
        We introduce CoVoST, a multilingual speech-to-text translation corpus from 11 languages into English,
        diversified with over 11,000 speakers and over 60 accents. We describe the dataset creation methodology and
        provide empirical evidence of the quality of the data. We also provide initial benchmarks, including, to our
        knowledge, the first end-to-end many-to-one multilingual models for spoken language translation. CoVoST is
        released under CC0 license and free to use. We also provide additional evaluation data derived from Tatoeba
        under CC licenses.`,
        "", "https://arxiv.org/pdf/2002.01320.pdf", "https://github.com/facebookresearch/covost"
    );
    addPaperItem(
        "aaai2020", "Neural Machine Translation with Byte-Level Subwords",
        ["Me"], ["Kyunghyun Cho", "Jiatao Gu"],
        "To appear in The Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI), 2020",
        `Almost all existing machine translation models are built on top of character-based vocabularies: characters,
        subwords or words. Rare characters from noisy text or character-rich languages such as Japanese and Chinese
        however can unnecessarily take up vocabulary slots and limit its compactness. Representing text at the level of
        bytes and using the 256 byte set as vocabulary is a potential solution to this issue. High computational cost
        has however prevented it from being widely deployed or used in practice. In this paper, we investigate
        byte-level subwords, specifically byte-level BPE (BBPE), which is compacter than character vocabulary and has
        no out-of-vocabulary tokens, but is more efficient than using pure bytes only is. We claim that contextualizing
        BBPE embeddings is necessary, which can be implemented by a convolutional or recurrent layer. Our experiments
        show that BBPE has comparable performance to BPE while its size is only 1/8 of that for BPE. In the multilingual
        setting, BBPE maximizes vocabulary sharing across many languages and achieves better translation quality.
        Moreover, we show that BBPE enables transferring models between languages with non-overlapping character sets.`,
        "static/aaai2020.png", "https://arxiv.org/pdf/1909.03341.pdf",
        "https://github.com/pytorch/fairseq/tree/master/examples/byte_level_bpe", "static/aaai2020_poster.pdf"
    );
    addPaperItem(
        "emnlp2019", "VizSeq: A Visual Analysis Toolkit for Text Generation Tasks",
        ["Me"], ["Anirudh Jain", "Danlu Chen", "Jiatao Gu"],
        "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP): System Demonstrations, 2019",
        `Automatic evaluation of text generation tasks (e.g. machine translation, text summarization, image captioning
        and video description) usually relies heavily on task-specific metrics, such as BLEU and ROUGE. They, however,
        are abstract numbers and are not perfectly aligned with human assessment. This suggests inspecting detailed
        examples as a complement to identify system error patterns. In this paper, we present VizSeq, a visual analysis
        toolkit for instance-level and corpus-level system evaluation on a wide variety of text generation tasks. It
        supports multimodal sources and multiple text references, providing visualization in Jupyter notebook or a web
        app interface. It can be used locally or deployed onto public servers for centralized data hosting and
        benchmarking. It covers most common n-gram based metrics accelerated with multiprocessing, and also provides
        latest embedding-based metrics such as BERTScore.`,
        "static/vizseq.png", "https://www.aclweb.org/anthology/D19-3043.pdf", "https://github.com/facebookresearch/vizseq",
        "static/emnlp2019_poster.pdf", "",
        "https://ai.facebook.com/blog/vizseq-a-visual-analysis-toolkit-for-accelerating-text-generation-research"
    );
    addPaperItem(
        "neurips2019", "Levenshtein Transformer",
        ["Jiatao Gu"], ["Me", "Jake Zhao"],
        "Advances in Neural Information Processing Systems (NeurIPS), 2019",
        `Modern neural sequence generation models are built to either generate tokens step-by-step from scratch or
        (iteratively) modify a sequence of tokens bounded by a fixed length. In this work, we develop Levenshtein
        Transformer, a new partially autoregressive model devised for more flexible and amenable sequence generation.
        Unlike previous approaches, the atomic operations of our model are insertion and deletion. The combination of
        them facilitates not only generation but also sequence refinement allowing dynamic length changes. We also
        propose a set of new training techniques dedicated at them, effectively exploiting one as the other's learning
        signal thanks to their complementary nature. Experiments applying the proposed model achieve comparable
        performance but much-improved efficiency on both generation (e.g. machine translation, text summarization) and
        refinement tasks (e.g. automatic post-editing). We further confirm the flexibility of our model by showing a
        Levenshtein Transformer trained by machine translation can straightforwardly be used for automatic post-editing.`,
        "static/neurips2019.png", "https://arxiv.org/pdf/1905.11006.pdf",
        "https://github.com/pytorch/fairseq/tree/master/examples/nonautoregressive_translation",
        "static/neurips2019_poster.pdf"
    );
    addPaperItem(
        "cvpr2019", "Does Object Recognition Work for Everyone?",
        ["Terrance DeVries", "Ishan Misra", "Me"], ["Laurens van der Maaten"],
        "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2019",
        `The paper analyzes the accuracy of publicly available object-recognition systems on a geographically diverse
        dataset. This dataset contains household items and was designed to have a more representative geographical
        coverage than commonly used image datasets in object recognition. We find that the systems perform relatively
        poorly on household items that commonly occur in countries with a low household income. Qualitative analyses
        suggest the drop in performance is primarily due to appearance differences within an object class
        (eg, dish soap) and due to items appearing in a different context (eg, toothbrushes appearing outside of
        bathrooms). The results of our study suggest that further work is needed to make object-recognition systems
        work equally well for people across different countries and income levels.`,
        "static/cvpr2019.png", "https://arxiv.org/pdf/1906.02659.pdf",
        "", "", "",
        "https://ai.facebook.com/blog/new-way-to-assess-ai-bias-in-object-recognition-systems/"
    );
    addPaperItem(
        "emnlp2018", "Dynamic Meta-Embeddings for Improved Sentence Representations",
        ["Douwe Kiela"], ["Me", "Kyunghyun Cho"],
        "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2018",
        `While one of the first steps in many NLP systems is selecting what pre-trained word embeddings to use, we
        argue that such a step is better left for neural networks to figure out by themselves. To that end, we
        introduce dynamic meta-embeddings, a simple yet effective method for the supervised learning of embedding
        ensembles, which leads to state-of-the-art performance within the same model class on a variety of tasks. We
        subsequently show how the technique can be used to shed new light on the usage of word embeddings in NLP
        systems.`,
        "static/emnlp2018.png", "https://arxiv.org/pdf/1804.07983.pdf", "https://github.com/facebookresearch/DME"
    );
    addPaperItem(
        "acl2018", "Code-Switched Named Entity Recognition with Embedding Attention",
        ["Me"], ["Kyunghyun Cho", "Douwe Kiela"],
        "Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching, 2018",
        `We describe our work for the CALCS 2018 shared task on named entity recognition on code-switched data. Our
        system ranked first place for MS Arabic-Egyptian named entity recognition and third place for English-Spanish.`,
        "static/calcs2018.png", "https://www.aclweb.org/anthology/W18-3221.pdf", "", "", "static/calcs2018_slides.pdf"
    );
    addPaperItem(
        "embc2015", "A Unified Framework for Automatic Wound Segmentation and Analysis with Deep Convolutional Neural Networks",
        ["Me"],
        ["Xinchen Yan", "Max Smith", "Kanika Kochhar", "Marcie Rubin", "Stephen M Warren", "James Wrobel",
            "Honglak Lee"],
        "The 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), 2015",
        `Wound surface area changes over multiple weeks are highly predictive of the wound healing process. Furthermore,
        the quality and quantity of the tissue in the wound bed also offer important prognostic information.
        Unfortunately, accurate measurements of wound surface area changes are out of reach in the busy wound practice
        setting. Currently, clinicians estimate wound size by estimating wound width and length using a scalpel after
        wound treatment, which is highly inaccurate. To address this problem, we propose an integrated system to
        automatically segment wound regions and analyze wound conditions in wound images. Different from previous
        segmentation techniques which rely on hand-crafted features or unsupervised approaches, our proposed deep
        learning method jointly learns task-relevant visual features and performs wound segmentation. Moreover, learned
        features are applied to further analysis of wounds in two ways: infection detection and healing progress
        prediction. To the best of our knowledge, this is the first attempt to automate long-term predictions of general
        wound healing progress. Our method is computationally efficient and takes less than 5 seconds per wound image
        (480 by 640 pixels) on a typical laptop computer. Our evaluations on a large-scale wound database demonstrate
        the effectiveness and reliability of the proposed system.`,
        "static/embc2015.png", "static/embc2015.pdf", "", "", "static/embc2015_slides.pdf"
    );
    addPaperItem(
        "cadcg2013", "InSide: Interactive Sketching for Image Database Exploration",
        ["Hongxin Zhang"], ["Dongyu Liu", "Me"],
        "International Conference on Computer-Aided Design and Computer Graphics (CAD/Graphics), 2013",
        `We propose an interactive sketching tool for exploring image database, called InSide. Our main contribution is
        a new solution of interactive image exploration that dynamically adapts to users' sketching and provides mixed
        feedback. A position-aware matching approach is proposed for InSide in order to support translation-free sketch
        searching. Based on demonstrated results, our method outperforms state-of-the-art approaches in aspects of user
        interface and matching results.`,
        "static/cadgraphics2013.png", "static/cadgraphics2013.pdf"
    );
</script>

</body>
</html>
